{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import os\n",
    "import os\n",
    "import sys\n",
    "\n",
    "## CHANGE THIS\n",
    "dir2 = os.path.abspath(\"/volume/Orion/orion\")\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path:\n",
    "    sys.path.append(dir1)\n",
    "import importlib\n",
    "\n",
    "importlib.util.find_spec(\"orion\")\n",
    "# import torch\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core.display import HTML, display\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.max_columns\", 250)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "pd.set_option(\"display.max_colwidth\", 300)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "def df_stats(df):\n",
    "    from tabulate import tabulate \n",
    "    print(\"\\n***** Shape: \", df.shape,\" *****\\n\")\n",
    "    \n",
    "    columns_list = df.columns.values.tolist()\n",
    "    isnull_list = df.isnull().sum().values.tolist()\n",
    "    isunique_list = df.nunique().values.tolist()\n",
    "    dtypes_list = df.dtypes.tolist()\n",
    "    \n",
    "    list_stat_val = list(zip(columns_list, isnull_list, isunique_list, dtypes_list))\n",
    "    df_stat_val = pd.DataFrame(list_stat_val, columns=['Name', 'Null', 'Unique', 'Dtypes'])\n",
    "    print(tabulate(df_stat_val, headers='keys', tablefmt='psql'))\n",
    "    return df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CathEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\n",
    "#    \"../data/train_val_test_LCA_REGRESSION_with_MHI_2021_data_NAS_path_mu.csv\",\n",
    "#    sep=\"µ\",\n",
    "#)\n",
    "#display(df.Split.value_counts())\n",
    "#df = df.loc[df['Split'] == 'VAL']\n",
    "#df.loc[df['Split'] == 'VAL', 'Split'] = 'inference'\n",
    "#df = df.drop(df.filter(regex='Unnamed').columns, axis=1)\n",
    "df = pd.read_csv('../data/inference_val_set_LCA_REGRESSION_with_MHI_2021_data_NAS_path_mu.csv', sep=\"µ\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orion.utils import video_run_inference\n",
    "checkpoints_folder = \"./swin3d_s_5_32_2_AdamW_new_20240120-151714_4iivz6og\" # path to the folder where the model checkpoints are stored\n",
    "data_path = '../data/inference_val_set_LCA_REGRESSION_with_MHI_2021_data_NAS_path_mu.csv'\n",
    "model_file_name = \"best.pt\" # name of the checkpoint file\n",
    "config_file_path = \"config/config_regression_swin3d.yaml\" # path to the model configuration file\n",
    "\n",
    "df_predictions_inference = video_run_inference.run_inference_and_no_logging(\n",
    "    checkpoints_folder=checkpoints_folder, \n",
    "    data_path=data_path,\n",
    "    model_file_name=model_file_name,\n",
    "    config_path=config_file_path,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orion.utils import video_run_inference\n",
    "checkpoints_folder = \"./swin3d_s_5_32_2_AdamW_new_20240120-151714_4iivz6og\" # path to the folder where the model checkpoints are stored\n",
    "model_file_name = \"best.pt\" # name of the checkpoint file\n",
    "wandb_id = \"4iivz6og\" # wandb run id, this comes from weights & biases website\n",
    "resume=True # If a model training run will be resumed or new training\n",
    "config_file_path = \"config/config_regression_swin3d.yaml\" # path to the model configuration file\n",
    "split='test' # evaluation mode\n",
    "\n",
    "df_predictions_inference = video_run_inference.run_inference_and_log_to_wandb(\n",
    "    checkpoints_folder=checkpoints_folder, \n",
    "    model_file_name=model_file_name,\n",
    "    wandb_id=wandb_id, \n",
    "    resume=resume, \n",
    "    config_path=config_file_path, \n",
    "    split=split\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load predictions\n",
    "df_predictions = pd.read_csv(checkpoints_folder + '/val_predictions.csv')\n",
    "\n",
    "# Rename columns y_hat to y_true and y_true to y_hat\n",
    "df_predictions = df_predictions.rename(columns={'y_hat': 'y_true', 'y_true': 'y_hat'})\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"../data/inference_val_set_LCA_REGRESSION_with_MHI_2021_data_NAS_path_mu.csv\",\n",
    "    sep=\"µ\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df.merge(df_predictions, left_on='FileName', right_on='filename')\n",
    "display(merged_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### some analyses, we then dichotomized this continuous prediction into ≤/>40%, since LVEF ≤40% defines significant left ventricular dysfunction associated and has been a common cutoff for HFrEF.\n",
    "### In sensitivity analysis, we also dichotomized using the <50%/≥50% cutoff which defines mildly reduced LVEF.5 More details in Supplement.\n",
    "\n",
    "grouped_df = merged_df.groupby(['Patient_ID', 'y_true']).agg({'Value': 'mean', 'y_hat': 'mean'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation\n",
    "correlation = merged_df['y_hat'].corr(merged_df['y_true'])\n",
    "print(\"Correlation:\", correlation)\n",
    "\n",
    "# Calculate AUC for y_true_40\n",
    "merged_df['y_true_40'] = np.where(merged_df['Value'] < 40, 0, 1)\n",
    "auc_40 = roc_auc_score(merged_df['y_true_40'], merged_df['y_hat'])\n",
    "print(\"AUC for y_true_40:\", auc_40)\n",
    "\n",
    "# Calculate AUC for y_true_50\n",
    "merged_df['y_true_50'] = np.where(merged_df['Value'] < 50, 0, 1)\n",
    "auc_50 = roc_auc_score(merged_df['y_true_50'], merged_df['y_hat'])\n",
    "print(\"AUC for y_true_50:\", auc_50)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "correlation = grouped_df['y_hat'].corr(grouped_df['y_true'])\n",
    "print(correlation)\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "\n",
    "grouped_df['y_true_40'] = np.where(grouped_df['Value'] < 40, 0, 1)\n",
    "auc = roc_auc_score(grouped_df['y_true_40'], grouped_df['y_hat'])\n",
    "print(\"AUC for 40\", auc)\n",
    "grouped_df['y_true_50'] = np.where(grouped_df['Value'] >= 50, 1, 0)\n",
    "auc = roc_auc_score(grouped_df['y_true_50'], grouped_df['y_hat'])\n",
    "print(\"AUC for 50\", auc)\n",
    "grouped_df['y_hat_class'] = np.where(grouped_df['y_hat'] >= 40, 1, 0)\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, title):\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Calculate and plot confusion matrix for y_true_40\n",
    "confusion_matrix_40 = confusion_matrix(grouped_df['y_true_40'], grouped_df['y_hat_class'])\n",
    "print(\"Confusion Matrix for y_true_40:\")\n",
    "print(confusion_matrix_40)\n",
    "plot_confusion_matrix(confusion_matrix_40, classes=['0', '1'], title='Confusion Matrix for y_true_40')\n",
    "\n",
    "# Calculate and plot confusion matrix for y_true_50\n",
    "confusion_matrix_50 = confusion_matrix(grouped_df['y_true_50'], grouped_df['y_hat_class'])\n",
    "print(\"Confusion Matrix for y_true_50:\")\n",
    "print(confusion_matrix_50)\n",
    "plot_confusion_matrix(confusion_matrix_50, classes=['0', '1'], title='Confusion Matrix for y_true_50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('/volume/Orion/swin3d_s_5_32_2_RAdam_new_20240103-032243/test_predictions.csv')\n",
    "# Remove square brackets and split the string into a list of floats\n",
    "df_test['y_hat'] = df_test['y_hat'].apply(lambda x: np.fromstring(x.strip('[]'), sep=' '))\n",
    "\n",
    "# Extract the argmax class for each row and convert to integer\n",
    "df_test['argmax_class'] = df_test['y_hat'].apply(lambda x: np.argmax(x).astype(int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "object_to_names = {\n",
    "    0: \"Aorta\",\n",
    "    1: \"Catheter\",\n",
    "    2: \"Femoral\",\n",
    "    3: \"Graft\",\n",
    "    4: \"LV\",\n",
    "    5: \"Left Coronary\",\n",
    "    6: \"Other\",\n",
    "    7: \"Pigtail\",\n",
    "    8: \"Radial\",\n",
    "    9: \"Right Coronary\",\n",
    "    10: \"Stenting\",\n",
    "}\n",
    "\n",
    "# Assuming df_test is your dataframe with 'y_hat' and y_true is your actual labels\n",
    "y_true = df_test['y_true'].astype(int)  # Ensure y_true is in integer format\n",
    "\n",
    "# One-hot encode y_true\n",
    "one_hot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "y_true_encoded = one_hot_encoder.fit_transform(y_true.values.reshape(-1, 1))\n",
    "\n",
    "# Plotting ROC curves for each class\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(y_true_encoded.shape[1]):\n",
    "    try:\n",
    "        fpr, tpr, _ = roc_curve(y_true_encoded[:, i], df_test['y_hat'].apply(lambda x: x[i]))\n",
    "        auc_score = roc_auc_score(y_true_encoded[:, i], df_test['y_hat'].apply(lambda x: x[i]))\n",
    "        plt.plot(fpr, tpr, label=f'{object_to_names[i]} (Class {i}) - AUC: {auc_score:.2f}')\n",
    "    except ValueError:\n",
    "        # Skip the class if error (like no variation in y_true labels for a class)\n",
    "        continue\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Dashed diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Each Class')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHANGE THIS\n",
    "dir2 = os.path.abspath(\"/volume/DicomVideoProcessing/downloadAvi\")\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path:\n",
    "    sys.path.append(dir1)\n",
    "\n",
    "from downloadAvi import plot_avi \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_test['y_hat'] contains the predicted probabilities\n",
    "# and object_to_names is your mapping dictionary\n",
    "\n",
    "# Determine the predicted class index for each row in y_hat\n",
    "df_test['predicted_class'] = df_test['y_hat'].apply(lambda x: np.argmax(x))\n",
    "\n",
    "# Map the predicted class indices to their respective names\n",
    "df_test['predicted_class_name'] = df_test['predicted_class'].map(object_to_names)\n",
    "df_test['true_class_name'] = df_test['y_true'].astype(float).astype(int).map(object_to_names)\n",
    "\n",
    "# Displaying the first few rows of the dataframe to verify the new column\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter the DataFrame where y_true is not equal to y_hat\n",
    "df_discordant = df_test[df_test['y_true'] == df_test['predicted_class']]\n",
    "\n",
    "# Group the DataFrame by 'predicted_class' and sample 5 examples from each group\n",
    "df_discordant = df_discordant.groupby('predicted_class').apply(lambda x: x.sample(min(5, len(x)), replace=False))\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "df_discordant = df_discordant.reset_index(drop=True)\n",
    "\n",
    "# Now call your plotting function\n",
    "plot_avi.sample_and_plot_middle_frames(df_discordant, 35, label_column='true_class_name', second_label_column='predicted_class_name', path_column='filename')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
