{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import os\n",
    "import os\n",
    "import sys\n",
    "\n",
    "## CHANGE THIS\n",
    "dir2 = os.path.abspath(\"/volume/Orion/orion\")\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path:\n",
    "    sys.path.append(dir1)\n",
    "import importlib\n",
    "\n",
    "importlib.util.find_spec(\"orion\")\n",
    "# import torch\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core.display import HTML, display\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.max_columns\", 250)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "pd.set_option(\"display.max_colwidth\", 300)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "def df_stats(df):\n",
    "    from tabulate import tabulate \n",
    "    print(\"\\n***** Shape: \", df.shape,\" *****\\n\")\n",
    "    \n",
    "    columns_list = df.columns.values.tolist()\n",
    "    isnull_list = df.isnull().sum().values.tolist()\n",
    "    isunique_list = df.nunique().values.tolist()\n",
    "    dtypes_list = df.dtypes.tolist()\n",
    "    \n",
    "    list_stat_val = list(zip(columns_list, isnull_list, isunique_list, dtypes_list))\n",
    "    df_stat_val = pd.DataFrame(list_stat_val, columns=['Name', 'Null', 'Unique', 'Dtypes'])\n",
    "    print(tabulate(df_stat_val, headers='keys', tablefmt='psql'))\n",
    "    return df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CathEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_folder = \"./swin3d_s_5_32_2_AdamW_new_20240122-184127_7hfxk75z\" # path to the folder where the model checkpoints are stored\n",
    "data_path = '../data/inference_val_set_LCA_REGRESSION_with_MHI_2021_data_NAS_path_mu.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to run inference on validation set and save results to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orion.utils import video_run_inference\n",
    "\n",
    "model_file_name = \"best.pt\" # name of the checkpoint file\n",
    "config_file_path = \"config/config_regression_swin3d.yaml\" # path to the model configuration file\n",
    "\n",
    "df_predictions_inference = video_run_inference.run_inference_and_no_logging(\n",
    "    checkpoints_folder=checkpoints_folder, \n",
    "    data_path=data_path,\n",
    "    model_file_name=model_file_name,\n",
    "    config_path=config_file_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions_inference.to_csv(checkpoints_folder + \"/df_predictions_inference_val.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to run inference on test set, log it to WANDB and save results to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orion.utils import video_run_inference\n",
    "checkpoints_folder = \"./swin3d_s_5_32_2_AdamW_new_20240120-151714_4iivz6og\" # path to the folder where the model checkpoints are stored\n",
    "model_file_name = \"best.pt\" # name of the checkpoint file\n",
    "wandb_id = \"4iivz6og\" # wandb run id, this comes from weights & biases website\n",
    "resume=True # If a model training run will be resumed or new training\n",
    "config_file_path = \"config/config_regression_swin3d.yaml\" # path to the model configuration file\n",
    "split='test' # evaluation mode\n",
    "\n",
    "df_predictions_inference = video_run_inference.run_inference_and_log_to_wandb(\n",
    "    checkpoints_folder=checkpoints_folder, \n",
    "    model_file_name=model_file_name,\n",
    "    wandb_id=wandb_id, \n",
    "    resume=resume, \n",
    "    config_path=config_file_path, \n",
    "    split=split\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore predictions and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_folder = \"./swin3d_s_5_32_2_AdamW_new_20240122-184127_7hfxk75z\" # path to the folder where the model checkpoints are stored\n",
    "data_path = '../data/inference_val_set_LCA_REGRESSION_with_MHI_2021_data_NAS_path_mu.csv'\n",
    "\n",
    "# load predictions\n",
    "df_predictions = pd.read_csv(checkpoints_folder + '/val_predictions.csv')\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"../data/inference_val_set_LCA_REGRESSION_with_MHI_2021_data_NAS_path_mu.csv\",\n",
    "    sep=\"µ\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df.merge(df_predictions, left_on='FileName', right_on='filename')\n",
    "merged_df['y_true'] = merged_df['Value'].astype(float)\n",
    "merged_df['dataset'] = merged_df['Patient_Name'].apply(lambda x: 'MHI' if pd.isnull(x) else 'UCSF')\n",
    "\n",
    "# Create 'y_true_cat' column based on the condition\n",
    "merged_df['y_true_cat'] = np.where(merged_df['Value'] <= 40, 'low ef', 'normal EF')\n",
    "merged_df['y_hat_cat'] = np.where(merged_df['y_hat'] <= 40, 'low ef', 'normal EF')\n",
    "merged_df['discordant_ef'] = np.where(merged_df['y_true_cat'] == merged_df['y_hat_cat'], 'no_discordant', 'yes_discordant')\n",
    "merged_df['y_true_cat'] = merged_df['y_true_cat'] + '_' + merged_df['dataset']\n",
    "merged_df['y_true_y_hat'] = merged_df['y_true'].astype(str) + '_Prediction:' + merged_df['y_hat'].astype(str)\n",
    "\n",
    "display(merged_df.tail(n=10))\n",
    "display(merged_df['dataset'].value_counts())\n",
    "display(merged_df['y_true_cat'].value_counts())\n",
    "display(merged_df.dicom_path.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir2 = os.path.abspath(\"/volume/DicomVideoProcessing/downloadAvi/\")\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path:\n",
    "    sys.path.append(dir1)\n",
    "import importlib\n",
    "\n",
    "importlib.util.find_spec(\"downloadAvi\")\n",
    "from downloadAvi import plot_avi as plot_avi\n",
    "\n",
    "merged_df_f = merged_df.loc[merged_df['discordant_ef'] == 'yes_discordant']\n",
    "#print(df_predictions.head(n=25))\n",
    "# Example usage\n",
    "# Assuming your DataFrame is named df\n",
    "plot_avi.sample_and_plot_middle_frames(merged_df_f, N=15, label_column='y_true_cat', second_label_column='y_true_y_hat', path_column='FileName')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### some analyses, we then dichotomized this continuous prediction into ≤/>40%, since LVEF ≤40% defines significant left ventricular dysfunction associated and has been a common cutoff for HFrEF.\n",
    "### In sensitivity analysis, we also dichotomized using the <50%/≥50% cutoff which defines mildly reduced LVEF.5 More details in Supplement.\n",
    "\n",
    "grouped_df = merged_df.groupby(['Patient_ID', 'dataset', 'y_true']).agg({'Value': 'mean', 'y_hat': 'mean'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_performance_metrics(df, thresholds=(40, 50), stratify_by='dataset', stratify_values=('UCSF', 'MHI')):\n",
    "    # Initialize a DataFrame to store performance metrics\n",
    "    performance_df = pd.DataFrame(columns=['Dataset', 'Metric', 'Value'])\n",
    "    \n",
    "    # Function to evaluate and store each metric\n",
    "    def evaluate_metric(dataset_name, metric_name, value):\n",
    "        # Create and return a one-row DataFrame\n",
    "        new_df = pd.DataFrame({'Dataset': [dataset_name], 'Metric': [metric_name], 'Value': [value]})\n",
    "        return new_df\n",
    "\n",
    "    # Stratify the dataframe if necessary and calculate metrics\n",
    "    for dataset in stratify_values + ('Overall',):\n",
    "        if dataset != 'Overall':\n",
    "            # Select the subset dataset\n",
    "            subset_df = df[df[stratify_by] == dataset]\n",
    "        else:\n",
    "            # Use the entire dataframe\n",
    "            subset_df = df\n",
    "\n",
    "        # Calculate the correlation\n",
    "        correlation = subset_df['y_hat'].corr(subset_df['y_true'])\n",
    "        performance_df = pd.concat([performance_df, evaluate_metric(dataset, 'Correlation', correlation)], ignore_index=True)\n",
    "        \n",
    "        # Calculate the AUC for various thresholds\n",
    "        for threshold in thresholds:\n",
    "            y_true_binary = np.where(subset_df['Value'] < threshold, 0, 1)\n",
    "            auc = roc_auc_score(y_true_binary, subset_df['y_hat'])\n",
    "            performance_df = pd.concat([performance_df, evaluate_metric(dataset, f'AUC for y_true_{threshold}', auc)], ignore_index=True)\n",
    "            \n",
    "            # If confusion matrix calculations are needed, they can be added here\n",
    "\n",
    "    return performance_df\n",
    "\n",
    "# Make sure merged_df and grouped_df are defined before calling this function\n",
    "\n",
    "# You can then use this function to calculate the metrics\n",
    "# (assuming merged_df and grouped_df are already defined):\n",
    "\n",
    "# Calculate metrics for the individual (merged) dataframe\n",
    "individual_performance = calculate_performance_metrics(merged_df)\n",
    "# Calculate metrics for the grouped dataframe\n",
    "grouped_performance = calculate_performance_metrics(grouped_df)\n",
    "\n",
    "# Optionally, concatenate the individual and grouped performance dataframes\n",
    "combined_performance = pd.concat([individual_performance, grouped_performance], ignore_index=True)\n",
    "\n",
    "# Print combined performance\n",
    "display(combined_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('/volume/Orion/swin3d_s_5_32_2_RAdam_new_20240103-032243/test_predictions.csv')\n",
    "# Remove square brackets and split the string into a list of floats\n",
    "df_test['y_hat'] = df_test['y_hat'].apply(lambda x: np.fromstring(x.strip('[]'), sep=' '))\n",
    "\n",
    "# Extract the argmax class for each row and convert to integer\n",
    "df_test['argmax_class'] = df_test['y_hat'].apply(lambda x: np.argmax(x).astype(int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "object_to_names = {\n",
    "    0: \"Aorta\",\n",
    "    1: \"Catheter\",\n",
    "    2: \"Femoral\",\n",
    "    3: \"Graft\",\n",
    "    4: \"LV\",\n",
    "    5: \"Left Coronary\",\n",
    "    6: \"Other\",\n",
    "    7: \"Pigtail\",\n",
    "    8: \"Radial\",\n",
    "    9: \"Right Coronary\",\n",
    "    10: \"Stenting\",\n",
    "}\n",
    "\n",
    "# Assuming df_test is your dataframe with 'y_hat' and y_true is your actual labels\n",
    "y_true = df_test['y_true'].astype(int)  # Ensure y_true is in integer format\n",
    "\n",
    "# One-hot encode y_true\n",
    "one_hot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "y_true_encoded = one_hot_encoder.fit_transform(y_true.values.reshape(-1, 1))\n",
    "\n",
    "# Plotting ROC curves for each class\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(y_true_encoded.shape[1]):\n",
    "    try:\n",
    "        fpr, tpr, _ = roc_curve(y_true_encoded[:, i], df_test['y_hat'].apply(lambda x: x[i]))\n",
    "        auc_score = roc_auc_score(y_true_encoded[:, i], df_test['y_hat'].apply(lambda x: x[i]))\n",
    "        plt.plot(fpr, tpr, label=f'{object_to_names[i]} (Class {i}) - AUC: {auc_score:.2f}')\n",
    "    except ValueError:\n",
    "        # Skip the class if error (like no variation in y_true labels for a class)\n",
    "        continue\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Dashed diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Each Class')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHANGE THIS\n",
    "dir2 = os.path.abspath(\"/volume/DicomVideoProcessing/downloadAvi\")\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path:\n",
    "    sys.path.append(dir1)\n",
    "\n",
    "from downloadAvi import plot_avi \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_test['y_hat'] contains the predicted probabilities\n",
    "# and object_to_names is your mapping dictionary\n",
    "\n",
    "# Determine the predicted class index for each row in y_hat\n",
    "df_test['predicted_class'] = df_test['y_hat'].apply(lambda x: np.argmax(x))\n",
    "\n",
    "# Map the predicted class indices to their respective names\n",
    "df_test['predicted_class_name'] = df_test['predicted_class'].map(object_to_names)\n",
    "df_test['true_class_name'] = df_test['y_true'].astype(float).astype(int).map(object_to_names)\n",
    "\n",
    "# Displaying the first few rows of the dataframe to verify the new column\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter the DataFrame where y_true is not equal to y_hat\n",
    "df_discordant = df_test[df_test['y_true'] == df_test['predicted_class']]\n",
    "\n",
    "# Group the DataFrame by 'predicted_class' and sample 5 examples from each group\n",
    "df_discordant = df_discordant.groupby('predicted_class').apply(lambda x: x.sample(min(5, len(x)), replace=False))\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "df_discordant = df_discordant.reset_index(drop=True)\n",
    "\n",
    "# Now call your plotting function\n",
    "plot_avi.sample_and_plot_middle_frames(df_discordant, 35, label_column='true_class_name', second_label_column='predicted_class_name', path_column='filename')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
